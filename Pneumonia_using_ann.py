# -*- coding: utf-8 -*-
"""Test_pneumonia (2).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VHWoiOkcrjGNkXHcJH2tARF18n0Rx4be
"""

# !git config --global user.name "YourGitHubUsername"
# !git config --global user.email "YourGitHubEmail"

"""# ***pneumonia detection using chest x ray in ANN***"""

import os
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import numpy as np
from numpy import random
import pandas as pd
from PIL import Image
import cv2
import ast
import math
from sklearn.model_selection import train_test_split

from google.colab import drive
drive.mount('/content/drive')
os.listdir('/content/drive/MyDrive/Colab/Data_set/Pneumonia/train')           #/IM-0666-0001.jpeg
file_path='/content/drive/MyDrive/Colab/Data_set/Pneumonia/train/'
data_set=['NORMAL', 'PNEUMONIA']
image=mpimg.imread(file_path+data_set[0]+'/IM-0115-0001.jpeg')
n_l=len(file_path+data_set[0])
p_l=len(file_path+data_set[1])
n_l,p_l
n_label=[0]*531
p_label=[1]*3883
print("n_label:",len(n_label))
print("p_label:",len(p_label))
print('normal labels:',n_label[0:5])
print('pneumonia labels:',p_label[0:5])

set_all=[]
N_set=[]
P_set=[]

"""# **IMAGE PREPROCESSING**

**set-1**
"""

n_f=file_path+data_set[0]
p_f=file_path+data_set[1]
n_f,p_f
for img in os.listdir(n_f):
  img=n_f+'/'+img
  try:
    img=cv2.imread(img,1)
    # print(img.shape)
    img=cv2.resize(img,(100,100))/255
    img=np.array(img)
    img=img.reshape(1,30000)
    N_set.append(img)
  except:
    continue
print(len(N_set))

"""**set-2**"""

for img in os.listdir(p_f):
  img=p_f+'/'+img
  try:
    img=cv2.imread(img,1)
    # print(img.shape)
    img=cv2.resize(img,(100,100))/255
    img=np.array(img)
    img=img.reshape(1,30000)
    P_set.append(img)
  except:
    continue
print(len(P_set))

set_all=np.concatenate((N_set,P_set),axis=0)
Y_labels=np.concatenate((n_label,p_label),axis=0)

# Splitting data into train and test sets (80-20 split)
X_train, X_test, y_train, y_test = train_test_split(set_all,Y_labels,test_size=0.2, random_state=2)

print(f"Training set size: {len(X_train)}, Test set size: {len(X_test),len(y_test)},len(y_train): {len(y_train)}")

"""# **ACTIVATION FUNCTION, RELU AND DERIVATIVES OF THESE FUNCTIONS**"""

def sigmoid(x):
  d=1/(1+np.exp(-x))
  # print("sigmoid:",d)
  return relu(d)
def relu(x):
  return np.maximum(0,x)
def der_sigmoid(x):
  return sigmoid(x)*(1-sigmoid(x))
def der_relu(x):
  return np.where(x>0,1,0)

"""# **Initalization of weights and bias**"""

def weight_assign():
  weight=np.random.uniform(-0.1,0.1,1920000)
  bias=np.zeros(64)
  weight=weight.reshape(30000,64)
  return weight,bias
# weight=np.random.uniform(-0.1,0.1,1920000)
# bias=np.random.uniform(-0.01,0.01,64)
# ler_rate=0.01
# exc_output=1

# set_all=np.array(set_all)
# print(weight.shape,set_all.shape)
# weight=weight.reshape(30000,64)
# set_all=np.array(set_all)
# set_all=set_all.reshape(4414,1,30000)
# print(weight.shape,set_all.shape)

"""# **Calculation of LOSS function and Derivative of LOSS**"""

def cal_loss(y_actual,y_pred):
  MSE=np.mean((y_pred-y_actual)**2)
  # print(f"MSE:{MSE}")
  return MSE
def cal_der_loss(y_actual,y_pred):
  return -(y_actual-y_pred)

"""# **Derivative of weights and bias**"""

def der_weight(w1,w2,w3,b1,b2,b3):
    d_weight1=w1
    d_weight2=w2
    d_weight3=w3
    d_bias1=b1
    d_bias2=b2
    d_bias3=b3
    print('WEights and bias derivative is sucess..!!')

"""# **Gradient function calculation**"""

def gre_weight(d_weight,d_error,d_sig):
  return d_weight*d_error*d_sig

"""# **WEIGHTS UPDATE In File**"""

def weight_update_file():
  with open("weight_update.txt",'w') as f:
    for i in range(30000):
      f.write(str(weight[i]))
      f.write("\n")
    for i in range(64):
      f.write(str(bias[i]))
      f.write("\n")

"""# **1-Weights and input multiplied and bias added**"""

def weight_multiply(weight,set_all):
  # print(set_all.shape,weight.shape)
  z=np.dot(set_all,weight)
  z=z+bias1
  # print("1-st layer shape:",z.shape)
  return z

"""# **2-Weights and bias with multiplication**"""

def layer_2nd(z_2nd):
  z_2nd=np.array(z_2nd)
  # print("Weights-2:",weight2.shape)
  # weight2=np.array(weight2)
  # print(type(weight2),type(z_2nd))
  # print(z_2nd.shape,weight2.shape)
  mul=np.dot(z_2nd,weight2)
  mul=mul+bias2
  # print("result-2:",mul.shape)
  return mul

"""# 3-Weights and bias with **multiplication**


"""

def layer_3rd(z_3rd):
  # print("3rd-layer:",weight3.shape,z_3rd.shape)
  mul=np.dot(z_3rd,weight3.T)
  # print(mul.shape,bias3.shape)
  mul=mul.T+bias3
  # print("result-3:",mul.shape)
  return mul

# set_all=N_set
# set_all[0].shape

"""# **forward-propagation**"""

# set_all[0]
# len(set_all)
for j in set_all[0:10]:
  print(j.shape)
  print(f"j={j}")
  break

global d_weight1
global d_weight2
global d_weight3
global d_bias1
global d_bias2
global d_bias3

# epochs=len(set_all)
epochs=100
learning_rate=0.001
pred=[]
for epoch in range(epochs):
  epoch_loss=0.0
  total_der_loss=0.0
  der_pred=[]
  try:
    with open("weight_update_1.txt",'r') as f:
      lines=f.readlines()
      print(f"length of lines={len(lines)}")
      weight1=float(lines[0].strip())
      bias1=float(lines[1].strip())
      weigth2=float(lines[2].strip())
      bias2=float(lines[3].strip())
      weight3=float(lines[4].strip())
      bias3=float(lines[5].strip())
      print(f"weight1={len(weight1)},bias1={len(bias1)}")
      print(f"weight2={len(weight2)},bias2={len(bias2)}")
      print(f"weight3={len(weight3)},bias3={len(bias3)}")
  except Exception as e:
    print(e)
    # weight-1 assign
    print("w not  found......")
    weight1,bias1=weight_assign()


    #weight -2 assign
    weight2=np.random.uniform(-0.1,0.1,(64,2))
    bias2=np.random.uniform(-0.01,0.01,2)

    #weight -3 assign
    weight3=np.random.uniform(-0.1,0.1,(1,2))
    bias3=np.random.uniform(-0.01,0.01,1)

  der_dw1_total=0.0
  der_dw2_total=0.0
  der_dw3_total=0.0
  der_db1_total=0.0
  der_db2_total=0.0
  der_db3_total=0.0
  l=len(X_train)

  for var in range(l):

    #forward propagation .........
    input_layer_1=weight_multiply(weight1,X_train[var])
    a1=relu(input_layer_1)

    input_layer_2=layer_2nd(input_layer_1)
    a2=relu(input_layer_2)

    input_layer_3=layer_3rd(input_layer_2)
    a3=relu(input_layer_3)



    #Backword propagation...........
    d_loss_3=cal_der_loss(y_train[var],a3)
    d_sig_1=der_sigmoid(input_layer_3)
    dz_dw3=a3
    der_loss_3=d_loss_3*d_sig_1

    # print(f"Der loss-3:{der_loss_3}")

    d_loss_2=der_sigmoid(input_layer_3)
    d_sig_2=der_sigmoid(input_layer_2)

    d_loss_1=der_sigmoid(input_layer_2)
    d_sig_3=der_sigmoid(input_layer_1)

    s=sigmoid(input_layer_3)
    d_s=der_sigmoid(input_layer_3)
    der_pred.append(d_s)
    pred.append(s)
    epoch_loss+=cal_loss(y_train[var],s)

    dloss_da3 = cal_der_loss(y_train[var], a3)
    da3_dz3 = der_sigmoid(input_layer_3)
    dz3_dw3 = a2

    dloss_dz3 = dloss_da3 * da3_dz3
    dloss_dw3 = np.dot(dz3_dw3.reshape(-1, 1), dloss_dz3.reshape(1, -1))
    dloss_db3 = dloss_dz3

    da2_dz2 = der_relu(input_layer_2)
    dz2_dw2 = a1
    # print(f"dloss_dz3 shape:{dloss_dz3.shape},  weight3 shape:{weight3.shape} ,da2_dz shape:{da2_dz2}")

    # dloss_dz2 = np.dot(dloss_dz3, weight3.T) * da2_dz2
    dloss_dz2 = np.dot(dloss_dz3, weight3) * da2_dz2

    dloss_dw2 = np.dot(dz2_dw2.reshape(-1, 1), dloss_dz2.reshape(1, -1))
    dloss_db2 = dloss_dz2

    da1_dz1 = der_relu(input_layer_1)
    dz1_dw1 = X_train[var].flatten()
    dloss_dz1 = np.dot(dloss_dz2, weight2.T) * da1_dz1
    dloss_dw1 = np.dot(dz1_dw1.reshape(-1, 1), dloss_dz1.reshape(1, -1))
    dloss_db1 = dloss_dz1

    # print(f"dz3_dw3.shape:{dz3_dw3.shape},,,,dloss_dz3.shape:{dloss_dz3.shape}")
    dloss_dw3 = np.dot(dz3_dw3.reshape(-1, 1), dloss_dz3.reshape(1, -1))

    if dloss_dw3.shape != weight3.shape:
      dloss_dw3 = dloss_dw3.reshape(weight3.shape)  # Match the shape

    der_dw1_total+=dloss_dw1
    der_dw2_total+=dloss_dw2
    der_dw3_total+=dloss_dw3
    der_db1_total+=dloss_db1
    der_db2_total+=dloss_db2
    der_db3_total+=dloss_db3



  der_dw1_total/=l
  der_dw2_total/=l
  der_dw3_total/=l
  der_db1_total/=l
  der_db2_total/=l
  der_db3_total/=l




  weight1-=learning_rate*dloss_dw1
  weight2-=learning_rate*dloss_dw2
  weight3-=learning_rate*dloss_dw3
  bias1-=learning_rate*dloss_db1.flatten()
  bias2-=learning_rate*dloss_db2.flatten()
  bias3-=learning_rate*dloss_db3.flatten()

  print(f"epoch of {epoch+1} loss is ={epoch_loss/l}")
  with open("weight_update_1.txt","w") as f:
    # for w,b in zip(weight1,bias1):
      # print(f"w={w.tolist()},b={b.tolist()}")
      # f.write(str(w.tolist())+'\n')
      # f.write(str(b.tolist())+'\n')
    f.write(str(weight1.tolist())+'\n')
    f.write(str(bias1.tolist())+'\n')
  # with open("weight_update_2.txt","w") as f:
    # for w,b in zip(weight2,bias2):
    #   f.write(str(w.tolist())+'\n')
    #   f.write(str(b.tolist())+'\n')
    f.write(str(weight2.tolist())+'\n')
    f.write(str(bias2.tolist())+'\n')
  # with open("weight_update_3.txt","w") as f:
    # for w,b in zip(weight3,bias3):
    #   f.write(str(w.tolist())+'\n')
    #   f.write(str(b.tolist())+'\n')
    f.write(str(weight3.tolist())+'\n')
    f.write(str(bias3.tolist())+'\n')

    print("Weight update is sucess...")
pred=np.array(pred)
print(pred.shape)
print(f"preducation of maximun :{np.max(pred)*100},minimam:{np.min(pred*100)}, average :{np.average(pred)*100}")